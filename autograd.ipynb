{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c34a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c629fe6",
   "metadata": {},
   "source": [
    "# `autograd` Usage in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n",
      "tensor(0.5421)\n",
      "torch.Size([1, 1000])\n",
      "tensor(0.2896)\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64) #  single 64x64 RGB image\n",
    "labels = torch.rand(1, 1000) # 1000 labels\n",
    "# print(model)\n",
    "print(data.shape)\n",
    "print(data[0,0,0,0])\n",
    "print(labels.shape)\n",
    "print(labels[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fec6dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# passing in a random \"image\" -- a \"3-channel 64x64 image\"\n",
    "prediction = model(data) # forward pass\n",
    "print(prediction.shape) # 1000 classes predicted against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6d4a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21936e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "575d8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08ccf9",
   "metadata": {},
   "source": [
    "# Differentiation in `autograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "343d501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([6., 4.], requires_grad=True)\n",
      "a.grad: None\n",
      "b.grad: None\n"
     ]
    }
   ],
   "source": [
    "# requires_grad=True signals to autograd that every operation on them should be tracked.\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "print(a)\n",
    "print(b)\n",
    "print(f'a.grad: {a.grad}')\n",
    "print(f'b.grad: {b.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6baaf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.,  65.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = 3*a**3 - b**2\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a5bc8",
   "metadata": {},
   "source": [
    "Let’s assume a and b to be parameters of an NN, and Q to be the error.  \n",
    "In NN training, we want gradients of the error w.r.t. parameters, i.e.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bcc3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f672f502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad: tensor([36., 81.])\n",
      "b.grad: tensor([-12.,  -8.])\n",
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(f'a.grad: {a.grad}')\n",
    "print(f'b.grad: {b.grad}')\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4ba712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to freeze params\n",
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "182e7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc = last linear layer\n",
    "# fc stands for fully connected, and it's the fully connected\n",
    "# layer at the end of the neural net model\n",
    "\n",
    "# Let’s say we want to finetune the model on a new dataset with 10 labels.\n",
    "# In resnet, the classifier is the last linear layer model.fc.\n",
    "# We can simply replace it with a new linear layer (unfrozen by default)\n",
    "# that acts as our classifier.\n",
    "model.fc = nn.Linear(512, 10)\n",
    "\n",
    "# Now all parameters in the model, except the parameters of model.fc, are frozen.\n",
    "# The only parameters that compute gradients are the weights and bias of model.fc.\n",
    "\n",
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_blitz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
